{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Emotion Classifier.ipynb","provenance":[{"file_id":"https://github.com/lavanyashukla/neural_networks/blob/master/Emotion%20Classifier%20-%20The%20Setup.ipynb","timestamp":1581182822924}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","language_info":{"name":"python","version":"3.6.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"cells":[{"cell_type":"markdown","metadata":{"id":"83pM76LxfXP8","colab_type":"text"},"source":["# Emotion Classifier - The Setup\n","\n","# Welcome!\n","In this notebook we'll train an emotion classifier and deploy it to a tensorflow js frontend.  The first step is setting up the environment.\n","\n","We’ll also set up Weights & Biases to log models metrics, inspect performance and share findings about the best architecture for the network. In this example we're using Google Colab as a convenient hosted environment, but you can run your own training scripts from anywhere and visualize metrics with W&B's experiment tracking tool.\n","\n","### Running This Notebook\n","1. Click \"Open in playground\" to create a copy of this notebook for yourself.\n","2. Save a copy in Google Drive for yourself.\n","3. To enable a GPU, please click Edit > Notebook Settings. Change the \"hardware accelerator\" to GPU.\n","4. Step through each section below, pressing play on the code blocks to run the cells.\n","\n","Results will be logged to a [shared W&B project page](https://app.wandb.ai/wandb/feb8-emotion).\n","\n","We highly encourage you to fork this notebook, tweak the parameters, or try the model with your own dataset!"]},{"cell_type":"code","metadata":{"id":"716z63DMfWoq","colab_type":"code","outputId":"510008fa-df9a-470b-fcd1-37b2a18bf8c7","executionInfo":{"status":"ok","timestamp":1581332234401,"user_tz":-330,"elapsed":26669,"user":{"displayName":"Huzefa Jambughoda","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjvMd9F-7O2DYggQHouh7tfp5CkuvUAH4pAOZm=s64","userId":"14187263876561245942"}},"colab":{"base_uri":"https://localhost:8080/","height":638}},"source":["# Install wandb\n","!pip install -qq wandb\n","!pip install opencv-python\n","!pip install tensorflow\n","!pip install portpicker"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 1.4MB 8.8MB/s \n","\u001b[K     |████████████████████████████████| 460kB 65.2MB/s \n","\u001b[K     |████████████████████████████████| 102kB 13.9MB/s \n","\u001b[K     |████████████████████████████████| 102kB 13.8MB/s \n","\u001b[K     |████████████████████████████████| 102kB 15.5MB/s \n","\u001b[K     |████████████████████████████████| 71kB 13.5MB/s \n","\u001b[K     |████████████████████████████████| 71kB 11.9MB/s \n","\u001b[?25h  Building wheel for gql (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for shortuuid (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for graphql-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (4.1.2.30)\n","Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python) (1.17.5)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (1.15.0)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.10.0)\n","Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.2)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.1.0)\n","Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.1)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.9.0)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.17.5)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.11.2)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.1.8)\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.8)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.34.2)\n","Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.1)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow) (45.1.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow) (3.1.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow) (0.16.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow) (2.8.0)\n","Requirement already satisfied: portpicker in /usr/local/lib/python3.6/dist-packages (1.2.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UdQHTvMTfs1c","colab_type":"code","colab":{}},"source":["#import libraries\n","import warnings\n","warnings.filterwarnings('ignore')\n","import pandas as pd\n","import numpy as np\n","import cv2\n","import subprocess\n","import os\n","import time\n","import wandb\n","os.environ['WANDB_NOTEBOOK_NAME'] = 'EmotionClassifier'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m6vgqwva0PKy","colab_type":"text"},"source":["## Load the fer2013 grayscale face emotion dataset\n","\n","https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data\n","\n","We manually do an 80/20 train-test split and cache the data to disk."]},{"cell_type":"code","metadata":{"id":"ujDGNN-FgNBa","colab_type":"code","colab":{}},"source":["def load_fer2013(force=False):\n","    \"\"\"Load the emotion dataset\"\"\"\n","    if force or not os.path.exists(\"fer2013\"):\n","        print(\"Downloading the face emotion dataset...\")\n","        subprocess.check_output(\n","            \"curl -SL https://www.dropbox.com/s/opuvvdv3uligypx/fer2013.tar | tar xz\", shell=True)\n","    print(\"Loading dataset...\")\n","    if not os.path.exists('face_cache.npz'):\n","        data = pd.read_csv(\"fer2013/fer2013.csv\")\n","        pixels = data['pixels'].tolist()\n","        width, height = 48, 48\n","        faces = []\n","        for pixel_sequence in pixels:\n","            pixs = pixel_sequence.split(' ')\n","            try:\n","                face = np.asarray(pixel_sequence.split(\n","                    ' '), dtype=np.uint8).reshape(width, height)\n","                face = cv2.resize(face.astype('uint8'), (width, height))\n","                faces.append(face.astype('float32'))\n","            except ValueError:\n","              print(\"Unable to load face.\")\n","\n","        faces = np.asarray(faces)\n","        faces = np.expand_dims(faces, -1)\n","        emotions = pd.get_dummies(data['emotion']).as_matrix()\n","\n","        val_faces = faces[int(len(faces) * 0.8):]\n","        val_emotions = emotions[int(len(faces) * 0.8):]\n","        train_faces = faces[:int(len(faces) * 0.8)]\n","        train_emotions = emotions[:int(len(faces) * 0.8)]\n","        np.savez('face_cache.npz', train_faces=train_faces, train_emotions=train_emotions,\n","                 val_faces=val_faces, val_emotions=val_emotions)\n","    cached = np.load('face_cache.npz')\n","\n","    return cached['train_faces'], cached['train_emotions'], cached['val_faces'], cached['val_emotions']"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ihedxZW-364Y","colab_type":"text"},"source":["# Deep Learning\n","\n","We define a train() function with default inputs.  In the second cell we manually call training and convert the keras model into a tensorflow js model."]},{"cell_type":"code","metadata":{"id":"u-E_wuM4gR9e","colab_type":"code","colab":{}},"source":["# Set default hyperparameters\n","default_config = {\n","    \"learning_rate\": 0.0001,\n","    \"batch_size\": 128,\n","    \"num_epochs\": 40,\n","    \"dropout\": 0.4\n","}\n","def train():\n","  \"\"\"Train an emotion classifier using wandb.config as input\"\"\"\n","  import tensorflow as tf\n","  import wandb\n","  tf.keras.backend.clear_session()\n","  # Inititialize W&B with default config options\n","  wandb.init(entity=\"wandb\", project=\"feb8-emotion\", config=default_config)\n","  config = wandb.config\n","  print(dict(config))\n","  \n","  # Load dataset\n","  input_shape = (48, 48, 1)\n","  train_faces, train_emotions, val_faces, val_emotions = load_fer2013()\n","  num_samples, num_classes = train_emotions.shape\n","  \n","  # Normalize data\n","  train_faces /= 255.\n","  val_faces /= 255.\n","  \n","  # Define the model\n","  optimizer = tf.keras.optimizers.Adam(lr=config.learning_rate)\n","  #model = tf.keras.applications.mobilenet_v2.MobileNetV2(input_shape=input_shape, include_top=False)\n","\n","  model = tf.keras.Sequential()\n","  \n","  # Set of Conv2D, Conv2D, MaxPooling2D layers with 32 and 64 filters\n","  model.add(tf.keras.layers.Conv2D(filters = 128, kernel_size = (4, 4), padding = 'same', \n","                    input_shape = input_shape))\n","  model.add(tf.keras.layers.LeakyReLU())\n","  model.add(tf.keras.layers.BatchNormalization())\n","  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n","  model.add(tf.keras.layers.Dropout(0.5))\n","\n","  model.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = (3, 3), padding = 'same', \n","                    input_shape = input_shape))\n","  model.add(tf.keras.layers.LeakyReLU())\n","  model.add(tf.keras.layers.BatchNormalization())\n","  \n","  model.add(tf.keras.layers.Conv2D(filters = 64, kernel_size = (3, 3), padding = 'same'))\n","  model.add(tf.keras.layers.LeakyReLU())\n","  model.add(tf.keras.layers.BatchNormalization())\n","  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n","  model.add(tf.keras.layers.Dropout(0.3))\n","\n","  model.add(tf.keras.layers.Flatten(input_shape=input_shape))\n","  \n","  model.add(tf.keras.layers.Dense(64, activation=\"relu\"))\n","  model.add(tf.keras.layers.Dense(32, activation=\"relu\"))\n","  model.add(tf.keras.layers.Dropout(0.4))\n","  model.add(tf.keras.layers.Dense(num_classes, activation=\"softmax\"))\n","  model.compile(optimizer=optimizer, loss='categorical_crossentropy',\n","                metrics=['accuracy'])\n","\n","  # Save extra hyperparameter\n","  config.total_params = model.count_params()\n","    \n","  # Train the model\n","  model.fit(train_faces, train_emotions, batch_size=config.batch_size,\n","            epochs=config.num_epochs, verbose=1, callbacks=[\n","                wandb.keras.WandbCallback(data_type=\"image\", labels=[\n","                              \"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"])\n","            ], validation_data=(val_faces, val_emotions))\n","\n","  # Save the model locally\n","#   model.save(\"emotion.h5\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ndvq2qYv66Ot","colab_type":"code","outputId":"6c3a5858-3541-4c00-8cf1-374f7e219049","executionInfo":{"status":"ok","timestamp":1581332557522,"user_tz":-330,"elapsed":347494,"user":{"displayName":"Huzefa Jambughoda","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAjvMd9F-7O2DYggQHouh7tfp5CkuvUAH4pAOZm=s64","userId":"14187263876561245942"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# Train the model\n","train()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://app.wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["wandb: ERROR Not authenticated.  Copy a key from https://app.wandb.ai/authorize\n"],"name":"stderr"},{"output_type":"stream","text":["API Key: ··········\n"],"name":"stdout"},{"output_type":"stream","text":["wandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n","                Project page: <a href=\"https://app.wandb.ai/wandb/feb8-emotion\" target=\"_blank\">https://app.wandb.ai/wandb/feb8-emotion</a><br/>\n","                Run page: <a href=\"https://app.wandb.ai/wandb/feb8-emotion/runs/cla1p3u9\" target=\"_blank\">https://app.wandb.ai/wandb/feb8-emotion/runs/cla1p3u9</a><br/>\n","            "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["{'learning_rate': 0.0001, 'batch_size': 128, 'num_epochs': 40, 'dropout': 0.4}\n","Downloading the face emotion dataset...\n","Loading dataset...\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","Train on 28709 samples, validate on 7178 samples\n","Epoch 1/40\n","28709/28709 [==============================] - 13s 458us/sample - loss: 1.9412 - acc: 0.2092 - val_loss: 1.9584 - val_acc: 0.2469\n","Epoch 2/40\n","28709/28709 [==============================] - 6s 225us/sample - loss: 1.8348 - acc: 0.2637 - val_loss: 1.8139 - val_acc: 0.2701\n","Epoch 3/40\n","28709/28709 [==============================] - 6s 226us/sample - loss: 1.7729 - acc: 0.2942 - val_loss: 1.6657 - val_acc: 0.3433\n","Epoch 4/40\n","28709/28709 [==============================] - 6s 226us/sample - loss: 1.7329 - acc: 0.3182 - val_loss: 1.6047 - val_acc: 0.3862\n","Epoch 5/40\n","28709/28709 [==============================] - 6s 225us/sample - loss: 1.6955 - acc: 0.3314 - val_loss: 1.5825 - val_acc: 0.3898\n","Epoch 6/40\n","28709/28709 [==============================] - 6s 225us/sample - loss: 1.6682 - acc: 0.3447 - val_loss: 1.5547 - val_acc: 0.3979\n","Epoch 7/40\n","28709/28709 [==============================] - 7s 227us/sample - loss: 1.6356 - acc: 0.3605 - val_loss: 1.5272 - val_acc: 0.4075\n","Epoch 8/40\n","28709/28709 [==============================] - 6s 225us/sample - loss: 1.6057 - acc: 0.3716 - val_loss: 1.5012 - val_acc: 0.4136\n","Epoch 9/40\n","28709/28709 [==============================] - 6s 226us/sample - loss: 1.5821 - acc: 0.3819 - val_loss: 1.4979 - val_acc: 0.4189\n","Epoch 10/40\n","28709/28709 [==============================] - 6s 225us/sample - loss: 1.5516 - acc: 0.3980 - val_loss: 1.4537 - val_acc: 0.4405\n","Epoch 11/40\n","28709/28709 [==============================] - 6s 225us/sample - loss: 1.5323 - acc: 0.4087 - val_loss: 1.4453 - val_acc: 0.4407\n","Epoch 12/40\n","28709/28709 [==============================] - 7s 227us/sample - loss: 1.5022 - acc: 0.4189 - val_loss: 1.4221 - val_acc: 0.4536\n","Epoch 13/40\n","28709/28709 [==============================] - 7s 227us/sample - loss: 1.4836 - acc: 0.4317 - val_loss: 1.4020 - val_acc: 0.4611\n","Epoch 14/40\n","28709/28709 [==============================] - 6s 226us/sample - loss: 1.4632 - acc: 0.4372 - val_loss: 1.3851 - val_acc: 0.4723\n","Epoch 15/40\n","28709/28709 [==============================] - 6s 225us/sample - loss: 1.4481 - acc: 0.4470 - val_loss: 1.3841 - val_acc: 0.4634\n","Epoch 16/40\n","28709/28709 [==============================] - 6s 224us/sample - loss: 1.4304 - acc: 0.4544 - val_loss: 1.4106 - val_acc: 0.4582\n","Epoch 17/40\n","28709/28709 [==============================] - 7s 227us/sample - loss: 1.4157 - acc: 0.4616 - val_loss: 1.3555 - val_acc: 0.4812\n","Epoch 18/40\n","28709/28709 [==============================] - 6s 225us/sample - loss: 1.3943 - acc: 0.4650 - val_loss: 1.3411 - val_acc: 0.4813\n","Epoch 19/40\n","28709/28709 [==============================] - 6s 224us/sample - loss: 1.3819 - acc: 0.4748 - val_loss: 1.3619 - val_acc: 0.4792\n","Epoch 20/40\n","28709/28709 [==============================] - 6s 223us/sample - loss: 1.3641 - acc: 0.4818 - val_loss: 1.3917 - val_acc: 0.4675\n","Epoch 21/40\n","28709/28709 [==============================] - 6s 225us/sample - loss: 1.3523 - acc: 0.4866 - val_loss: 1.3363 - val_acc: 0.4914\n","Epoch 22/40\n","28709/28709 [==============================] - 7s 228us/sample - loss: 1.3360 - acc: 0.4947 - val_loss: 1.3368 - val_acc: 0.4822\n","Epoch 23/40\n","28709/28709 [==============================] - 6s 224us/sample - loss: 1.3271 - acc: 0.4995 - val_loss: 1.3951 - val_acc: 0.4698\n","Epoch 24/40\n","28709/28709 [==============================] - 6s 225us/sample - loss: 1.3157 - acc: 0.5024 - val_loss: 1.3192 - val_acc: 0.5011\n","Epoch 25/40\n","28709/28709 [==============================] - 6s 224us/sample - loss: 1.2955 - acc: 0.5111 - val_loss: 1.3338 - val_acc: 0.4943\n","Epoch 26/40\n","28709/28709 [==============================] - 7s 227us/sample - loss: 1.2827 - acc: 0.5169 - val_loss: 1.3044 - val_acc: 0.5021\n","Epoch 27/40\n","28709/28709 [==============================] - 6s 224us/sample - loss: 1.2719 - acc: 0.5197 - val_loss: 1.3161 - val_acc: 0.5036\n","Epoch 28/40\n","28709/28709 [==============================] - 6s 224us/sample - loss: 1.2578 - acc: 0.5271 - val_loss: 1.3584 - val_acc: 0.4795\n","Epoch 29/40\n","28709/28709 [==============================] - 6s 225us/sample - loss: 1.2468 - acc: 0.5287 - val_loss: 1.2988 - val_acc: 0.5029\n","Epoch 30/40\n","28709/28709 [==============================] - 6s 225us/sample - loss: 1.2352 - acc: 0.5382 - val_loss: 1.2829 - val_acc: 0.5153\n","Epoch 31/40\n","28709/28709 [==============================] - 6s 225us/sample - loss: 1.2264 - acc: 0.5410 - val_loss: 1.3562 - val_acc: 0.4889\n","Epoch 32/40\n","28709/28709 [==============================] - 6s 224us/sample - loss: 1.2079 - acc: 0.5496 - val_loss: 1.3216 - val_acc: 0.5031\n","Epoch 33/40\n","28709/28709 [==============================] - 6s 225us/sample - loss: 1.1977 - acc: 0.5497 - val_loss: 1.2729 - val_acc: 0.5202\n","Epoch 34/40\n","28709/28709 [==============================] - 6s 224us/sample - loss: 1.1858 - acc: 0.5561 - val_loss: 1.2893 - val_acc: 0.5248\n","Epoch 35/40\n","28709/28709 [==============================] - 6s 226us/sample - loss: 1.1726 - acc: 0.5592 - val_loss: 1.2622 - val_acc: 0.5177\n","Epoch 36/40\n","28709/28709 [==============================] - 6s 225us/sample - loss: 1.1602 - acc: 0.5655 - val_loss: 1.2554 - val_acc: 0.5293\n","Epoch 37/40\n","28709/28709 [==============================] - 6s 225us/sample - loss: 1.1506 - acc: 0.5709 - val_loss: 1.2643 - val_acc: 0.5228\n","Epoch 38/40\n","28709/28709 [==============================] - 6s 224us/sample - loss: 1.1409 - acc: 0.5709 - val_loss: 1.2862 - val_acc: 0.5191\n","Epoch 39/40\n","28709/28709 [==============================] - 6s 222us/sample - loss: 1.1265 - acc: 0.5795 - val_loss: 1.2655 - val_acc: 0.5261\n","Epoch 40/40\n","28709/28709 [==============================] - 6s 223us/sample - loss: 1.1137 - acc: 0.5834 - val_loss: 1.2666 - val_acc: 0.5273\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yRbQA5Zw55Yr","colab_type":"text"},"source":["# Setup & serve the frontend\n","\n","We're downloading and serving a pre-built React application from [github](https://github.com/vanpelt/emotion-detector)"]},{"cell_type":"code","metadata":{"id":"UhAYYxTzo9vA","colab_type":"code","colab":{}},"source":["# Download the frontend build\n","!rm -rf build\n","!wget -q https://github.com/vanpelt/emotion-detector/releases/download/stable/frontend.zip\n","!unzip -q frontend.zip"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6IMdxcE3aRZF","colab_type":"code","outputId":"0a481988-0fa4-4cd5-cc35-41070660b116","colab":{"base_uri":"https://localhost:8080/","height":714}},"source":["# Install tensorflowjs in a virtualenv\n","%pip install -q virtualenv\n","!virtualenv --no-site-packages venv && . venv/bin/activate && pip install -q tensorflowjs"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 3.4MB 3.5MB/s \n","\u001b[?25hUsing base prefix '/usr'\n","New python executable in /content/venv/bin/python3\n","Also creating executable in /content/venv/bin/python\n","Installing setuptools, pip, wheel...\n","done.\n","\u001b[K     |████████████████████████████████| 56 kB 2.0 MB/s \n","\u001b[K     |████████████████████████████████| 20.1 MB 19 kB/s \n","\u001b[K     |████████████████████████████████| 104.6 MB 49 kB/s \n","\u001b[K     |████████████████████████████████| 2.9 MB 52.5 MB/s \n","\u001b[K     |████████████████████████████████| 89 kB 11.2 MB/s \n","\u001b[K     |████████████████████████████████| 248 kB 61.4 MB/s \n","\u001b[K     |████████████████████████████████| 896 kB 50.8 MB/s \n","\u001b[K     |████████████████████████████████| 689 kB 46.8 MB/s \n","\u001b[K     |████████████████████████████████| 26.1 MB 13 kB/s \n","\u001b[K     |████████████████████████████████| 57 kB 4.8 MB/s \n","\u001b[K     |████████████████████████████████| 69 kB 8.3 MB/s \n","\u001b[K     |████████████████████████████████| 41 kB 890 kB/s \n","\u001b[K     |████████████████████████████████| 448 kB 64.1 MB/s \n","\u001b[K     |████████████████████████████████| 2.7 MB 49.6 MB/s \n","\u001b[K     |████████████████████████████████| 1.3 MB 47.7 MB/s \n","\u001b[K     |████████████████████████████████| 3.8 MB 46.1 MB/s \n","\u001b[K     |████████████████████████████████| 50 kB 7.9 MB/s \n","\u001b[K     |████████████████████████████████| 104 kB 74.3 MB/s \n","\u001b[K     |████████████████████████████████| 88 kB 10.4 MB/s \n","\u001b[K     |████████████████████████████████| 76 kB 6.6 MB/s \n","\u001b[K     |████████████████████████████████| 298 kB 68.4 MB/s \n","\u001b[K     |████████████████████████████████| 57 kB 6.6 MB/s \n","\u001b[K     |████████████████████████████████| 155 kB 83.9 MB/s \n","\u001b[K     |████████████████████████████████| 133 kB 76.7 MB/s \n","\u001b[K     |████████████████████████████████| 125 kB 82.1 MB/s \n","\u001b[K     |████████████████████████████████| 156 kB 79.5 MB/s \n","\u001b[K     |████████████████████████████████| 58 kB 8.0 MB/s \n","\u001b[K     |████████████████████████████████| 147 kB 68.2 MB/s \n","\u001b[K     |████████████████████████████████| 77 kB 7.2 MB/s \n","\u001b[?25h  Building wheel for PyInquirer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for opt-einsum (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for absl-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qBYv-QDl2f4G","colab_type":"code","colab":{}},"source":["# Quantize our trained model\n","!. venv/bin/activate && tensorflowjs_converter --input_format keras --quantization_bytes 2 emotion.h5 build/models"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hRpqPdPQp1OK","colab_type":"code","colab":{}},"source":["# Serve our custom UI\n","from subprocess import Popen\n","import portpicker\n","try:\n","  server.kill()\n","except NameError:\n","  pass\n","port = portpicker.pick_unused_port()\n","server = Popen([\"cd ./build && python -m http.server %i\" % port], shell=True,\n","               stdin=None, stdout=None, stderr=None, close_fds=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"H3kHfhxpj3PE","colab_type":"code","colab":{}},"source":["#Setup the interface for display\n","import IPython\n","html = open(\"./build/index.html\").read()\n","body = html.replace('=\"/', '=\"https://localhost:{}/'.format(port),10)\n","body = body.replace(\"</head>\", '<script type=\"text/javascript\"/>window.BASE_URL = \"https://localhost:{}/\";google.colab.output.setIframeHeight(600)</script></head>'.format(port))\n","display(IPython.display.HTML(body))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qbFBh3PE97j8","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}